{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "mhdd93op13D_",
      "metadata": {
        "id": "mhdd93op13D_"
      },
      "source": [
        "# Podcast Transcript Summarizer and Recommendation Engine\n",
        "\n",
        "This notebook covers:\n",
        "1. Fetching podcast transcripts.\n",
        "2. Summarizing transcripts using an LLM.\n",
        "3. Keyword search within transcripts.\n",
        "4. Content recommendation engine using embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mbzd5IBD13ED",
      "metadata": {
        "id": "Mbzd5IBD13ED"
      },
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Install required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_extxZx313EE",
      "metadata": {
        "id": "_extxZx313EE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install requests bs4 transformers sentence-transformers scikit-learn\n",
        "!pip install datasets transformers\n",
        "!git clone https://github.com/FelipeGRK/theamericanlifepodcast.git\n",
        "!pip install datasets transformers ipywidgets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_GbVvbck13EG",
      "metadata": {
        "id": "_GbVvbck13EG"
      },
      "source": [
        "## 2. Import Libraries\n",
        "\n",
        "Import necessary libraries and modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cep_60Oy13EM",
      "metadata": {
        "id": "cep_60Oy13EM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from getpass import getpass\n",
        "\n",
        "# === Data Handling & Processing ===\n",
        "from datasets import Dataset\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# === Transformers & Hugging Face Utilities ===\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from huggingface_hub import InferenceClient, login\n",
        "\n",
        "# === Interactive Widgets & Display ===\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lFnpEIWo13EO",
      "metadata": {
        "id": "lFnpEIWo13EO"
      },
      "source": [
        "## 3. Authenticate with Hugging Face\n",
        "\n",
        "Sign in to Hugging Face using an API Key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dlNsWIvH13EP",
      "metadata": {
        "id": "dlNsWIvH13EP"
      },
      "outputs": [],
      "source": [
        "hf_api_key = getpass(\"Please enter your Hugging Face API key: \")\n",
        "login(token=hf_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZOfeAaLf13EW",
      "metadata": {
        "id": "ZOfeAaLf13EW"
      },
      "source": [
        "## 5. Fetch Podcast Transcripts\n",
        "\n",
        "Fetch transcripts from provided URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J1Lct6KI13EX",
      "metadata": {
        "id": "J1Lct6KI13EX"
      },
      "outputs": [],
      "source": [
        "def fetch_transcript(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        transcript_div = soup.find(\"div\", class_=\"transcript\")\n",
        "        if transcript_div:\n",
        "            transcript_text = transcript_div.get_text(separator=\"\\n\")\n",
        "        else:\n",
        "            transcript_text = soup.get_text(separator=\"\\n\")\n",
        "        return transcript_text.strip()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "transcript_urls = [\n",
        "    \"https://www.thisamericanlife.org/1/transcript\",\n",
        "    \"https://www.thisamericanlife.org/2/transcript\",\n",
        "    \"https://www.thisamericanlife.org/3/transcript\",\n",
        "    \"https://www.thisamericanlife.org/4/transcript\",\n",
        "    \"https://www.thisamericanlife.org/5/transcript\",\n",
        "    \"https://www.thisamericanlife.org/6/transcript\",\n",
        "    \"https://www.thisamericanlife.org/7/transcript\",\n",
        "    \"https://www.thisamericanlife.org/8/transcript\",\n",
        "    \"https://www.thisamericanlife.org/9/transcript\",\n",
        "    \"https://www.thisamericanlife.org/10/transcript\",\n",
        "]\n",
        "\n",
        "transcripts = []\n",
        "for url in transcript_urls:\n",
        "    transcript_text = fetch_transcript(url)\n",
        "    if transcript_text:\n",
        "        transcripts.append(transcript_text)\n",
        "        print(f\"Transcript fetched from {url}\")\n",
        "    else:\n",
        "        print(f\"Failed to retrieve transcript from {url}\")\n",
        "\n",
        "if transcripts:\n",
        "    print(\"Transcripts fetched successfully.\")\n",
        "else:\n",
        "    print(\"Failed to fetch transcripts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vKzCW8oR13Ed",
      "metadata": {
        "id": "vKzCW8oR13Ed"
      },
      "source": [
        "## 7. Define a Custom Prompt\n",
        "\n",
        "Create a function to generate custom prompts for summarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jSsEWJVf13Ed",
      "metadata": {
        "id": "jSsEWJVf13Ed"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(transcript, episode, title, date):\n",
        "    prompt = f\"\"\"\n",
        "You are an assistant specialized in summarizing podcast episodes.\n",
        "From the following transcript, generate a concise and informative summary that includes:\n",
        "- Episode number: {episode}\n",
        "- Title: {title}\n",
        "- Publication date: {date}\n",
        "- Main topics discussed in the episode\n",
        "- Names of speakers and guests mentioned\n",
        "\n",
        "Transcript:\n",
        "{transcript}\n",
        "\n",
        "Respond with a clear and structured summary.\n",
        "\"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eKD5bc-r13Ee",
      "metadata": {
        "id": "eKD5bc-r13Ee"
      },
      "source": [
        "## 8. Create Prompts for the First 10 Episodes\n",
        "\n",
        "Construct prompts using the fetched transcripts and metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A7VzkjbU13Ej",
      "metadata": {
        "id": "A7VzkjbU13Ej"
      },
      "outputs": [],
      "source": [
        "metadata = [\n",
        "    {\"episode\": \"001\", \"title\": \"New Beginnings\", \"date\": \"95-11-17\"},\n",
        "    {\"episode\": \"002\", \"title\": \"Small Scale Sin\", \"date\": \"95-11-24\"},\n",
        "    {\"episode\": \"003\", \"title\": \"A Violent Utopia\", \"date\": \"95-12-01\"},\n",
        "    {\"episode\": \"004\", \"title\": \"Animals\", \"date\": \"95-12-08\"},\n",
        "    {\"episode\": \"005\", \"title\": \"Anger and Forgiveness\", \"date\": \"95-12-15\"},\n",
        "    {\"episode\": \"006\", \"title\": \"Poultry Slam 1995\", \"date\": \"95-12-22\"},\n",
        "    {\"episode\": \"007\", \"title\": \"Quitting\", \"date\": \"96-01-05\"},\n",
        "    {\"episode\": \"008\", \"title\": \"On Work\", \"date\": \"96-01-12\"},\n",
        "    {\"episode\": \"009\", \"title\": \"Julia Sweeney\", \"date\": \"96-01-19\"},\n",
        "    {\"episode\": \"010\", \"title\": \"Double Lives\", \"date\": \"96-01-26\"},\n",
        "]\n",
        "\n",
        "prompts = []\n",
        "for i, transcript in enumerate(transcripts):\n",
        "    prompt_custom = generate_prompt(transcript, metadata[i][\"episode\"], metadata[i][\"title\"], metadata[i][\"date\"])\n",
        "    prompts.append(prompt_custom)\n",
        "    print(f\"Custom prompt created for episode {metadata[i]['episode']}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t5IkbECT4Nv6",
      "metadata": {
        "id": "t5IkbECT4Nv6"
      },
      "source": [
        "6. Select Podcast Transcript\n",
        "Allow the user to select the podcast transcript they want to see and provide an option to listen to the podcast.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EGlnSJCd4H3Q",
      "metadata": {
        "collapsed": true,
        "id": "EGlnSJCd4H3Q"
      },
      "outputs": [],
      "source": [
        "def display_transcript_options(transcripts, metadata):\n",
        "    for i, meta in enumerate(metadata):\n",
        "        print(f\"{i+1}. Title: {meta['title']} - Publication date: {meta['date']}\")\n",
        "\n",
        "    selection = int(input(\"Select the transcript you want to see (1-10): \")) - 1\n",
        "    if 0 <= selection < len(transcripts):\n",
        "        print(f\"Showing transcript for episode: {metadata[selection]['title']}\")\n",
        "        print(transcripts[selection])\n",
        "    else:\n",
        "        print(\"Invalid selection.\")\n",
        "\n",
        "display_transcript_options(transcripts, metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Summarize the Transcripts\n",
        "Loading and Generating summaries for each transcript.\n"
      ],
      "metadata": {
        "id": "X8_d496o-wIn"
      },
      "id": "X8_d496o-wIn"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Path to the folder\n",
        "folder_path = \"/content/theamericanlifepodcast/transcript-text\"\n",
        "\n",
        "# Function to split text chunks for summarization\n",
        "def chunk_text(text, max_chunk_chars=1024):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 > max_chunk_chars:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "            current_length = len(word) + 1\n",
        "        else:\n",
        "            current_chunk.append(word)\n",
        "            current_length += len(word) + 1\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "# Function to summarize a transcript from my dataset\n",
        "def summarize_transcript(batch):\n",
        "    text = batch[\"transcript\"]\n",
        "    summary = \"\"\n",
        "    chunks = chunk_text(text, max_chunk_chars=1024)\n",
        "    for chunk in chunks:\n",
        "        # Adjust summarization parameters\n",
        "        input_length = len(chunk.split())\n",
        "        max_len = min(200, int(input_length * 0.8))\n",
        "        min_len = min(50, int(input_length * 0.4))\n",
        "\n",
        "        generated = summarizer(chunk, max_length=max_len, min_length=min_len, do_sample=False)\n",
        "        summary += generated[0]['summary_text'] + \" \"\n",
        "    batch[\"summary\"] = summary.strip()\n",
        "    return batch\n",
        "\n",
        "# Read all .txt files from the folder and create a dataset\n",
        "files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".txt\")])\n",
        "data = {\"filename\": [], \"transcript\": []}\n",
        "\n",
        "for file_name in files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data[\"filename\"].append(file_name)\n",
        "        data[\"transcript\"].append(file.read())\n",
        "\n",
        "# Create a dataset from the loaded data\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "# Preview the dataset\n",
        "print(\"Dataset loaded. Preview:\")\n",
        "print(dataset)\n",
        "\n",
        "# Apply the summarization function to each record in the dataset\n",
        "dataset = dataset.map(summarize_transcript)\n",
        "\n",
        "# Print the summaries for all files\n",
        "for record in dataset:\n",
        "    print(\"Filename:\", record[\"filename\"])\n",
        "    print(\"Generated Summary:\\n\", record[\"summary\"])\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "h-3RwrBMl6xX"
      },
      "id": "h-3RwrBMl6xX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dropdown list to Select a Podcast for Summarization\n",
        "LOADING PODCAST TRANSCCRIPTS BASED THE USER CHOICE"
      ],
      "metadata": {
        "id": "UmxTO7PRLlzs"
      },
      "id": "UmxTO7PRLlzs"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell: Interactive Dropdown to Display Summaries\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Prepare dropdown options\n",
        "options = [(rec[\"filename\"], i) for i, rec in enumerate(dataset)]\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=options,\n",
        "    description='Podcast:',\n",
        "    value=options[0][1]  # Default to the first transcript\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_selection_change(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        selected_index = change['new']\n",
        "        output.clear_output()\n",
        "        # Show a brief \"Loading\" indicator\n",
        "        with output:\n",
        "            print(\"Loading summarized version, please wait...\\n\")\n",
        "        # Retrieve the recorrd\n",
        "        selected_record = dataset[selected_index]\n",
        "        output.clear_output()\n",
        "        with output:\n",
        "            print(\"Filename:\", selected_record[\"filename\"])\n",
        "            print(\"Generated Summary:\\n\", selected_record[\"summary\"])\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "dropdown.observe(on_selection_change, names='value')\n",
        "display(dropdown, output)"
      ],
      "metadata": {
        "id": "jWpnDapnsoRf"
      },
      "id": "jWpnDapnsoRf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "jszaXIP413Eq",
      "metadata": {
        "id": "jszaXIP413Eq"
      },
      "source": [
        "## 10. Keyword Search Engine\n",
        "\n",
        "Implement a keyword search engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2hycsMd13Eq",
      "metadata": {
        "id": "a2hycsMd13Eq"
      },
      "outputs": [],
      "source": [
        "def keyword_search(transcript, keyword):\n",
        "    lines = transcript.split('\\n')\n",
        "    results = [line for line in lines if keyword.lower() in line.lower()]\n",
        "    return results\n",
        "\n",
        "if 'transcripts' in globals():\n",
        "    keyword = \"betray\"\n",
        "    search_results = keyword_search(transcripts[1], keyword)\n",
        "\n",
        "    print(f\"Search results for '{keyword}':\\n\")\n",
        "    for result in search_results:\n",
        "        print(result)\n",
        "else:\n",
        "    print(\"Transcripts are not defined. Please run the fetching cell .\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j0YBGti_13Es",
      "metadata": {
        "id": "j0YBGti_13Es"
      },
      "source": [
        "## 11. Semantic Search with Embeddings\n",
        "\n",
        "Implement a semantic search engine using embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cw4clYOG13Es",
      "metadata": {
        "id": "Cw4clYOG13Es"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "transcript_embeddings = model.encode(transcripts)\n",
        "\n",
        "def semantic_search(query, embeddings, top_k=5):\n",
        "    query_embedding = model.encode([query])[0]\n",
        "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
        "    similar_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    return similar_indices, similarities\n",
        "\n",
        "query = \"economic impact\"\n",
        "similar_indices, similarities = semantic_search(query, transcript_embeddings)\n",
        "\n",
        "print(\"Semantic search results for query \\\"economic impact\\\":\\n\")\n",
        "for idx in similar_indices:\n",
        "    print(f\"Episode {metadata[idx]['episode']}, Similarity: {similarities[idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2yoG1s-13Ex",
      "metadata": {
        "id": "f2yoG1s-13Ex"
      },
      "source": [
        "## 12. Content Recommendation Engine\n",
        "\n",
        "Implement a content recommendation engine using embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UpX1DD8E13Ex",
      "metadata": {
        "id": "UpX1DD8E13Ex"
      },
      "outputs": [],
      "source": [
        "def recommend_episodes(transcript_embedding, all_embeddings, top_k=5):\n",
        "    similarities = cosine_similarity([transcript_embedding], all_embeddings)[0]\n",
        "    similar_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    return similar_indices, similarities\n",
        "\n",
        "example_transcript_embedding = transcript_embeddings[1]\n",
        "similar_indices, similarities = recommend_episodes(example_transcript_embedding, transcript_embeddings)\n",
        "\n",
        "print(\"Recommended episodes based on similarity:\\n\")\n",
        "for idx in similar_indices:\n",
        "    print(f\"Episode {metadata[idx]['episode']}, Similarity: {similarities[idx]:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}