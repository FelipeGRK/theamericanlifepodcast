{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Podcast Transcript Summarizer and Recommendation System\n",
        "\n",
        "In this notebook, I will:\n",
        "- Clone a Git repository containing the following folders/files:\n",
        "  - `episode-html` – HTML files for each episode (main episode pages)\n",
        "  - `transcript-text` – Text files with transcript content for each episode\n",
        "  - `transcript-html` – HTML files of the full transcript pages\n",
        "  - `episodes.html` – The main page with episodes listings\n",
        "  - `episodes_info.csv` – Episode metadata (columns: ep_num, ep_name, pub_date, host, contributers, num_acts, last_timestamp, url_suffix)\n",
        "- Load and parse the data.\n",
        "- Perform exploratory data analysis (EDA) on the transcripts and metadata.\n",
        "- Use prompt engineering with a Hugging Face LLM (e.g., T5-base) to generate short summaries for each transcript so that users can preview the episode content.\n",
        "- (Optionally) Set up a basic recommendation engine using article metadata and keywords.\n",
        "\n",
        "**Solution Space Overview:**\n",
        "\n",
        "- **Keyword Search Engine**  \n",
        "  *Pros:* Simple implementation.  \n",
        "  *Cons:* Limited by exact-match limitations.\n",
        "\n",
        "- **LLM-Based Summarization**  \n",
        "  *Pros:* Quickly generates episode summaries for fast comprehension.  \n",
        "  *Cons:* May need fine-tuning to avoid inaccuracies.\n",
        "\n",
        "- **Semantic Search with Embeddings**  \n",
        "  *Pros:* Supports natural language queries for better search relevance.  \n",
        "  *Cons:* Requires careful tuning and compute resources.\n",
        "\n",
        "- **Content Recommendation Engine**  \n",
        "  *Pros:* Enhances exploration by suggesting similar episodes.  \n",
        "  *Cons:* Risks recommending irrelevant content if embeddings aren’t well-calibrated.\n",
        "\n",
        "In this notebook, I focus on LLM-based summarization (and later, recommendation) using my existing data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zyi3U1ohdwba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Clone the Git Repository and List Files\n",
        "\n",
        "In this cell, I clone the Git repository that contains all the folders and files.\n"
      ],
      "metadata": {
        "id": "ZyI3Yg0Ld5bd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s1buMp04d7IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone your Git repository (adjust the URL to your repo)\n",
        "!git clone https://github.com/FelipeGRK/theamericanlifepodcast.git\n",
        "\n",
        "# Change directory into the repository\n",
        "%cd yourrepo\n",
        "\n",
        "# List the directory structure to confirm that folders/files are present\n",
        "!find . -maxdepth=2 | sort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTRor3IBd9S4",
        "outputId": "ac256603-71ed-4969-ef41-1447966eb8c8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'theamericanlifepodcast' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: 'yourrepo'\n",
            "/content\n",
            "find: unknown predicate `-maxdepth=2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Episode Metadata and Transcripts\n",
        "\n",
        "Here, I load the episode metadata from `episodes_info.csv` and read the transcripts from the `transcript-text` folder. The metadata file should include columns such as:  \n",
        "- ep_num, ep_name, pub_date, host, contributers, num_acts, last_timestamp, url_suffix\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ermL3mnd_lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the paths to your local folders/files\n",
        "transcript_folder = \"transcript-text\"   # Folder with text transcripts (e.g., \"1.txt\", \"2.txt\", etc.)\n",
        "episode_info_file = \"episodes_info.csv\"   # CSV file with episode metadata\n",
        "\n",
        "# Load the episode metadata\n",
        "episode_info = pd.read_csv(episode_info_file)\n",
        "print(\"Episode Metadata:\")\n",
        "display(episode_info.head())\n",
        "\n",
        "# Function to load a transcript for a given episode number\n",
        "def load_transcript(ep_num):\n",
        "    file_path = os.path.join(transcript_folder, f\"{ep_num}.txt\")\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            return f.read().strip()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Add a transcript column to the metadata DataFrame\n",
        "episode_info['transcript'] = episode_info['ep_num'].apply(lambda x: load_transcript(x))\n",
        "\n",
        "# Display a preview of transcript for Episode 2\n",
        "ep2 = episode_info[episode_info['ep_num'] == 2]\n",
        "if not ep2.empty:\n",
        "    print(\"Transcript for Episode 2 (preview):\")\n",
        "    print(ep2['transcript'].values[0][:500])\n",
        "else:\n",
        "    print(\"Episode 2 transcript not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "6DzrSmLteFwF",
        "outputId": "025731d6-b684-427a-e53b-f5b5be40a382"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'episodes_info.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-2859fa82a3c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the episode metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mepisode_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode Metadata:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'episodes_info.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Now I analyze the transcripts' characteristics, such as transcript length distribution.\n"
      ],
      "metadata": {
        "id": "VRwjENGQeH6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vj2mWsmaeKgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate transcript lengths (in words)\n",
        "episode_info['transcript_length'] = episode_info['transcript'].apply(lambda x: len(x.split()) if x else 0)\n",
        "\n",
        "# Plot a histogram of transcript lengths\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(episode_info['transcript_length'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Transcript Lengths')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6BAHKxQeLmi",
        "outputId": "073cb56d-3910-4204-f086-e23d80f2690f",
        "collapsed": true
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript fetched successfully. Preview:\n",
            "2: Small Scale Sin - This American Life\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Hi. We love you. Be our Life Partner.\n",
            "\n",
            "\n",
            "Support the show to get ad-free listening, bonus content, and our new Greatest Hits Archive.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Learn more\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "00:00\n",
            "\n",
            "\n",
            "00:00\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Transcript\n",
            "\n",
            "\n",
            "Share\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "This American Life\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Life Partners...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generate Short Summaries Using LLM and Prompt Engineering\n",
        "\n",
        "In this section, I build a custom prompt that combines the transcript with episode metadata (episode number, title, publication date, host, and contributors). This prompt is then used with a Hugging Face model (e.g., T5-base) to generate a short summary.\n",
        "\n",
        "If you require an API key, you can provide it as shown.\n",
        "\n"
      ],
      "metadata": {
        "id": "OnuAEtfZePUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install transformers if not installed (already done above if necessary)\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Option 1: If you need to use your Hugging Face API key, uncomment and update:\n",
        "# summarizer = pipeline(\"text2text-generation\", model=\"t5-base\", use_auth_token=\"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\")\n",
        "\n",
        "# Option 2: For public models:\n",
        "summarizer = pipeline(\"text2text-generation\", model=\"t5-base\", use_auth_token=\"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\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoXm7nq4eTvU",
        "outputId": "827e0d77-5788-4ec7-bb72-8171b6566322"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Define the Prompt Function\n",
        "\n",
        "This function constructs a prompt that instructs the model to summarize the transcript and include the episode metadata.\n",
        "\n"
      ],
      "metadata": {
        "id": "bkiuHDBCeXOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(transcript, ep_num, ep_name, pub_date, host, contribuidores):\n",
        "    prompt = f\"\"\"\n",
        "You are an assistant specialized in summarizing podcast episodes.\n",
        "Based on the following transcript, generate a concise and informative summary that includes:\n",
        "- Episode Number: {ep_num}\n",
        "- Title: {ep_name}\n",
        "- Publication Date: {pub_date}\n",
        "- Host: {host}\n",
        "- Contributors: {contribuidores}\n",
        "- Main topics discussed, speakers, and guests mentioned\n",
        "\n",
        "Transcript:\n",
        "{transcript}\n",
        "\n",
        "Please respond with a clear and structured summary.\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "v8CEmyzteaK0"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Build the Custom Prompt for Episode 2\n",
        "\n",
        "Using the metadata and transcript for Episode 2, I create the custom prompt.\n"
      ],
      "metadata": {
        "id": "bOR59wJZeddc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TjK6YgRdelWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Episode 2 data\n",
        "ep2_info = episode_info[episode_info['ep_num'] == 2].iloc[0]\n",
        "transcript_ep2 = ep2_info['transcript']\n",
        "\n",
        "if transcript_ep2:\n",
        "    custom_prompt = generate_prompt(\n",
        "        transcript=transcript_ep2,\n",
        "        ep_num=ep2_info['ep_num'],\n",
        "        ep_name=ep2_info['ep_name'],\n",
        "        pub_date=ep2_info['pub_date'],\n",
        "        host=ep2_info['host'],\n",
        "        contribuidores=ep2_info['contributers']\n",
        "    )\n",
        "    print(\"Custom Prompt for Episode 2 (preview):\\n\")\n",
        "    print(custom_prompt[:500], \"...\\n\")\n",
        "else:\n",
        "    print(\"Transcript for Episode 2 not found. Cannot generate prompt.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "ue7aPqLoefaR",
        "outputId": "781aae59-5a19-478a-9f74-414ea8c27a15"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'episode_info' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-7eb8e1bd4e08>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Retrieve Episode 2 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mep2_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepisode_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ep_num'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtranscript_ep2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mep2_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transcript'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtranscript_ep2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'episode_info' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. (Optional) Recommend Related Episodes\n",
        "\n",
        "For a simple content recommendation, I can use the episode metadata (such as keywords in the title) and similarity metrics.  \n",
        "Below is an example of a simple recommendation function based on matching keywords between episodes.\n"
      ],
      "metadata": {
        "id": "L2H_2IrDem9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_episodes(target_ep_num, metadata_df, top_n=3):\n",
        "    \"\"\"\n",
        "    Recommend related episodes based on similar keywords in the title.\n",
        "    This is a simple approach: it compares lowercase words in the episode titles.\n",
        "    \"\"\"\n",
        "    target_title = metadata_df[metadata_df['ep_num'] == target_ep_num]['ep_name'].values[0].lower()\n",
        "    target_keywords = set(target_title.split())\n",
        "\n",
        "    recommendations = []\n",
        "    for _, row in metadata_df.iterrows():\n",
        "        if row['ep_num'] == target_ep_num:\n",
        "            continue\n",
        "        title = row['ep_name'].lower()\n",
        "        keywords = set(title.split())\n",
        "        common = target_keywords.intersection(keywords)\n",
        "        score = len(common)\n",
        "        recommendations.append((row['ep_num'], row['ep_name'], score))\n",
        "\n",
        "    # Sort recommendations by score in descending order\n",
        "    recommendations = sorted(recommendations, key=lambda x: x[2], reverse=True)\n",
        "    return recommendations[:top_n]\n",
        "\n",
        "# Example: Recommend episodes related to Episode 2\n",
        "recommended = recommend_episodes(2, episode_info, top_n=3)\n",
        "print(\"Recommended Episodes for Episode 2:\")\n",
        "for rec in recommended:\n",
        "    print(f\"Episode {rec[0]} - {rec[1]} (Score: {rec[2]})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "r9Hja_wezdqc",
        "outputId": "25c3e43e-fa55-4416-8eba-ee2c863ea989"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'episode_info' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-2169c5fe95df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Example: Recommend episodes related to Episode 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mrecommended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommend_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recommended Episodes for Episode 2:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecommended\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'episode_info' is not defined"
          ]
        }
      ]
    }
  ]
}