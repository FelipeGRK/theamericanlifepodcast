#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
TAL Podcast Data Processing and Summarization Script

This script performs the following tasks:

Notebook 1: TAL EDA and Data Scraping
--------------------------------------
1. Loads episode metadata from 'episode_info.csv'.
2. Loads the transcripts for each episode from the 'transcript-txt' folder.
3. Performs exploratory data analysis (EDA) by visualizing the distribution of transcript lengths.

Notebook 2: TAL Summarization with LLM and Prompt Engineering
--------------------------------------------------------------
1. Builds a custom prompt using episode metadata and transcript content.
2. Uses a Hugging Face LLM (e.g., t5-base) to generate a short summary for a selected episode.
   The summary includes the episode number, title, publication date, host, contributors, and main topics.

Before running, install the required packages:
    pip install pandas matplotlib transformers

If you need to use your Hugging Face API key, you can either set it as an environment variable
or include it in the pipeline initialization.
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
from transformers import pipeline

###############################
# Notebook 1: Data Scraping and EDA
###############################

# Define paths to data folders and files
transcript_folder = "transcript-txt"  # Folder with transcript .txt files (e.g., "1.txt", "2.txt", etc.)
episode_info_file = "episode_info.csv"  # CSV file with episode metadata

# Load episode metadata
episode_info = pd.read_csv(episode_info_file)
print("Episode Information:")
print(episode_info.head())

# Function to load a transcript given an episode number
def load_transcript(ep_num):
    file_path = os.path.join(transcript_folder, f"{ep_num}.txt")
    if os.path.exists(file_path):
        with open(file_path, encoding="utf-8") as f:
            return f.read().strip()
    else:
        return None

# Add a new column 'transcript' by loading each transcript based on ep_num
episode_info['transcript'] = episode_info['ep_num'].apply(lambda x: load_transcript(x))

# Preview transcript for episode 2
ep2 = episode_info[episode_info['ep_num'] == 2]
if not ep2.empty:
    print("\nEpisode 2 Transcript (preview):")
    print(ep2['transcript'].values[0][:500])
else:
    print("Episode 2 not found.")

# EDA: Analyze distribution of transcript lengths
episode_info['transcript_length'] = episode_info['transcript'].apply(lambda x: len(x.split()) if x else 0)
plt.hist(episode_info['transcript_length'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Transcript Lengths')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')
plt.show()

###############################
# Notebook 2: Summarization with LLM and Prompt Engineering
###############################

# If needed, install transformers and import pipeline (already done above)

# Initialize the summarization pipeline.
# If you need to use an API key, you can pass it via use_auth_token.
# Example:
# summarizer = pipeline("text2text-generation", model="t5-base", use_auth_token="YOUR_API_KEY")
summarizer = pipeline("text2text-generation", model="t5-base")

# Define a function to build a custom prompt using episode metadata and transcript content
def gerar_prompt(transcript, ep_num, ep_name, pub_date, host, contribuidores):
    prompt = f"""
You are an assistant specialized in summarizing podcast episodes.
Based on the following transcript, generate a concise and informative summary that includes:
- Episode Number: {ep_num}
- Title: {ep_name}
- Publication Date: {pub_date}
- Host: {host}
- Contributors: {contribuidores}
- Main topics discussed, speakers, and guests mentioned

Transcript:
{transcript}

Please respond with a clear and structured summary.
"""
    return prompt

# Build the prompt and generate summary for Episode 2

# Retrieve metadata for episode 2 (assuming ep_num is numeric in the CSV)
try:
    ep2_info = episode_info[episode_info['ep_num'] == 2].iloc[0]
    transcript_ep2 = ep2_info['transcript']
except IndexError:
    transcript_ep2 = None

if transcript_ep2:
    prompt_custom = gerar_prompt(
        transcript=transcript_ep2,
        ep_num=ep2_info['ep_num'],
        ep_name=ep2_info['ep_name'],
        pub_date=ep2_info['pub_date'],
        host=ep2_info['host'],
        contribuidores=ep2_info['contributers']
    )
    print("\nCustom Prompt (preview):")
    print(prompt_custom[:500], "...\n")
    
    # Generate the summary using the LLM
    resumo_gerado = summarizer(prompt_custom, max_length=200, truncation=True)
    print("Generated Summary for Episode 2:\n")
    print(resumo_gerado[0]['generated_text'])
else:
    print("Transcript for Episode 2 not found; cannot generate summary.")

###############################
# End of Script
###############################

# Optional: Save the updated DataFrame with transcripts and analysis to a CSV file.
output_csv = "episode_info_with_transcripts.csv"
episode_info.to_csv(output_csv, index=False)
print(f"\nEpisode information with transcripts saved to {output_csv}")
