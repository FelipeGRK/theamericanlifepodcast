{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overview-md",
   "metadata": {},
   "source": [
    "# Proposed Solution Overview\n",
    "\n",
    "In this project, I am building a podcast discovery system that combines several components:\n",
    "\n",
    "- **LLM-Based Summarization:** Quickly generates short summaries for each transcript so users can preview the episode content. _Pros:_ Fast and informative summaries. _Cons:_ May need prompt engineering or fine-tuning.\n",
    "\n",
    "- **Content Recommendation Engine:** Uses article metadata (episode number, title, publication date, etc.) combined with keywords to recommend related episodes. _Pros:_ Enhances exploration by suggesting similar content. _Cons:_ Requires calibration to avoid irrelevant recommendations.\n",
    "\n",
    "- **Keyword Search Engine:** A simple search over transcripts based on exact keyword matching. _Pros:_ Easy to implement. _Cons:_ Limited by exact-match requirements.\n",
    "\n",
    "Below, I implement LLM-based summarization and a simple recommendation engine using my local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load Episode Metadata and Transcripts\n",
    "# -----------------------------\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "transcript_folder = \"transcript-txt\"  # Folder with transcript text files (e.g., \"1.txt\", \"2.txt\", etc.)\n",
    "episode_info_file = \"episodes_info.csv\"  # CSV file with episode metadata\n",
    "\n",
    "# Load episode metadata\n",
    "episode_info = pd.read_csv(episode_info_file)\n",
    "print(\"Episode Metadata:\")\n",
    "display(episode_info.head())\n",
    "\n",
    "# Function to load a transcript given an episode number\n",
    "def load_transcript(ep_num):\n",
    "    file_path = os.path.join(transcript_folder, f\"{ep_num}.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Add a transcript column to the metadata DataFrame\n",
    "episode_info['transcript'] = episode_info['ep_num'].apply(lambda x: load_transcript(x))\n",
    "\n",
    "# Preview transcript for Episode 2\n",
    "ep2 = episode_info[episode_info['ep_num'] == 2]\n",
    "if not ep2.empty:\n",
    "    print(\"Transcript for Episode 2 (preview):\")\n",
    "    print(ep2['transcript'].values[0][:500])\n",
    "else:\n",
    "    print(\"Episode 2 transcript not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-md",
   "metadata": {},
   "source": [
    "## LLM-Based Summarization\n",
    "\n",
    "This section uses prompt engineering to generate a short summary for an episode. The custom prompt will include the transcript along with key metadata (episode number, title, publication date, host, contributors) so users can preview the content before listening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-summarization",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the summarization pipeline.\n",
    "# If you need to use your Hugging Face API key, uncomment the following line and replace YOUR_API_KEY:\n",
    "# summarizer = pipeline(\"text2text-generation\", model=\"t5-base\", use_auth_token=\"YOUR_API_KEY\")\n",
    "summarizer = pipeline(\"text2text-generation\", model=\"t5-base\")\n",
    "\n",
    "# Define a function to build a custom prompt\n",
    "def generate_prompt(transcript, ep_num, ep_name, pub_date, host, contribuidores):\n",
    "    prompt = f\"\"\"\n",
    "You are an assistant specialized in summarizing podcast episodes.\n",
    "Based on the following transcript, generate a concise and informative summary that includes:\n",
    "- Episode Number: {ep_num}\n",
    "- Title: {ep_name}\n",
    "- Publication Date: {pub_date}\n",
    "- Host: {host}\n",
    "- Contributors: {contribuidores}\n",
    "- Main topics discussed, speakers, and guests mentioned\n",
    "\n",
    "Transcript:\n",
    "{transcript}\n",
    "\n",
    "Please respond with a clear and structured summary.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Retrieve data for Episode 2\n",
    "ep2_info = episode_info[episode_info['ep_num'] == 2].iloc[0]\n",
    "transcript_ep2 = ep2_info['transcript']\n",
    "\n",
    "if transcript_ep2:\n",
    "    custom_prompt = generate_prompt(\n",
    "        transcript=transcript_ep2,\n",
    "        ep_num=ep2_info['ep_num'],\n",
    "        ep_name=ep2_info['ep_name'],\n",
    "        pub_date=ep2_info['pub_date'],\n",
    "        host=ep2_info['host'],\n",
    "        contribuidores=ep2_info['contributers']\n",
    "    )\n",
    "    print(\"Custom Prompt for Episode 2 (preview):\\n\")\n",
    "    print(custom_prompt[:500], \"...\\n\")\n",
    "    \n",
    "    # Generate summary using the LLM\n",
    "    summary_output = summarizer(custom_prompt, max_length=200, truncation=True)\n",
    "    print(\"Generated Summary for Episode 2:\\n\")\n",
    "    print(summary_output[0]['generated_text'])\n",
    "else:\n",
    "    print(\"Transcript for Episode 2 not found. Cannot generate summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rec-md",
   "metadata": {},
   "source": [
    "## Content Recommendation Engine\n",
    "\n",
    "This section experiments with recommending related episodes based on article metadata and keyword overlap in the episode titles. This simple approach splits titles into keywords and scores episodes based on the number of common keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recommendation-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_episodes(target_ep_num, metadata_df, top_n=3):\n",
    "    # Get the title of the target episode\n",
    "    target_title = metadata_df[metadata_df['ep_num'] == target_ep_num]['ep_name'].values[0].lower()\n",
    "    target_keywords = set(target_title.split())\n",
    "    \n",
    "    recommendations = []\n",
    "    for _, row in metadata_df.iterrows():\n",
    "        if row['ep_num'] == target_ep_num:\n",
    "            continue\n",
    "        title = row['ep_name'].lower()\n",
    "        keywords = set(title.split())\n",
    "        common = target_keywords.intersection(keywords)\n",
    "        score = len(common)\n",
    "        recommendations.append((row['ep_num'], row['ep_name'], score))\n",
    "    \n",
    "    # Sort recommendations by score in descending order\n",
    "    recommendations = sorted(recommendations, key=lambda x: x[2], reverse=True)\n",
    "    return recommendations[:top_n]\n",
    "\n",
    "# Example: Recommend episodes related to Episode 2\n",
    "recommended = recommend_episodes(2, episode_info, top_n=3)\n",
    "print(\"Recommended Episodes for Episode 2:\")\n",
    "for rec in recommended:\n",
    "    print(f\"Episode {rec[0]} - {rec[1]} (Score: {rec[2]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keyword-md",
   "metadata": {},
   "source": [
    "## Keyword Search Engine\n",
    "\n",
    "As a simple baseline, this section implements a keyword search that scans the transcript texts for a given query. Although limited to exact matches, it serves as a comparison for more advanced semantic search methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "keyword-search-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(query, metadata_df):\n",
    "    query = query.lower()\n",
    "    results = []\n",
    "    for _, row in metadata_df.iterrows():\n",
    "        transcript = row['transcript']\n",
    "        if transcript and query in transcript.lower():\n",
    "            results.append((row['ep_num'], row['ep_name']))\n",
    "    return results\n",
    "\n",
    "# Example: Search for the keyword \"sin\" in transcripts\n",
    "search_results = keyword_search(\"sin\", episode_info)\n",
    "print(\"Keyword Search Results for 'sin':\")\n",
    "for res in search_results:\n",
    "    print(f\"Episode {res[0]} - {res[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "url-suffix-md",
   "metadata": {},
   "source": [
    "## URL Suffix Generation\n",
    "\n",
    "This cell defines a function to scrub punctuation from a name, remove extra whitespace, and replace inner whitespace with hyphens to generate a URL suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-url-suffix-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Scrubbing punctuation from name, removing whitespace and replacing inner whitespace with hyphens\n",
    "def get_url_suffix(name):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation + \"’‘…\")\n",
    "    clean = name.translate(translator)\n",
    "    clean = re.sub(\"—\", \" \", clean).strip()\n",
    "    clean = re.sub(\" - \", \" \", clean).strip()\n",
    "    clean = re.sub(\"ä\", \"a\", clean).strip()\n",
    "    print(clean)\n",
    "    return re.sub(r\" +\", \" \", clean).strip().replace(\" \", \"-\")\n",
    "\n",
    "# Example usage\n",
    "suffix = get_url_suffix(\"The Problem We All Live With Part One\")\n",
    "print(suffix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
